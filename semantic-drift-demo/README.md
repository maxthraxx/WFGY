
# Semanticâ€‘Drift-Demo

30 multiâ€‘step prompts that expose how answers drift off topic.  
We compare plain LLM output (**Baseline**) to **WFGYâ€¯+â€¯Drunkâ€¯Mode**.

| Metric | Meaning | Good? |
|--------|---------|-------|
| **Î”S** | Promptâ€‘toâ€‘answer distance (0Â =Â perfect) | â†“ lower |
| **Î»_observe** | % of cases with Î”SÂ <Â thresholdÂ (0.4) | â†‘ higher |

<div align="center">
  <img src="images/drift_comparison.png" width="420"/>
  <img src="images/lambda_pass.png"  width="420"/>
</div>

---

## QuickÂ Start

```bash
pip install -r requirements.txt          # sklearn, pandas, matplotlib, statsmodels
python scripts/run_eval.py               # â†’ data/metrics.csv
python scripts/plot_results.py           # â†’ images/ charts refreshed
````

### Swap in your own answers

1. Edit `data/baseline_answers.txt` and `data/wfgydrunk_answers.txt`
   â€“ one answer block per prompt, blank line between.
2. Run the two commands above â€” charts update automatically.

### Optional: Îºâ€¯agreement

Fill `data/error_annotations.csv` with rater votes (`ok` / `drift`), then:

```bash
python scripts/compute_kappa.py
```

---

## Folder layout

```
semantic-drift-demo/
â”œâ”€ data/        prompts & answers
â”œâ”€ scripts/     *.py utilities
â”œâ”€ images/      output charts
â””â”€ requirements.txt
```

---

### Concept recap

* Î”S measures *how far* an answer strays from the promptâ€™s meaning.
* Î»\_observe shows *how often* WFGY keeps that distance below the safe limit.

  > In our demo WFGY hits 100% â€” every answer stays on track.

Unzip the archive â†’ push to GitHub â†’ readers can rerun in Colab and reproduce the same numbers. Thatâ€™s the entire experiment pipeline.

```

Just unzip one of the links into your `semantic-drift-demo/` folder, paste the README block above, and commit â€” youâ€™re good to go! ðŸŽ¯
```
