<!-- ======================================================= -->
<!--  memory-design-patterns.md Â· Semantic Clinic / Map-E    -->
<!--  Draft v0.1 Â· MIT Â· 2025-08-06                         -->
<!--  Purpose: Show repeatable patterns for â€œcross-session   -->
<!--  memoryâ€ without creating uncontrolled context bloat.   -->
<!-- ======================================================= -->

# ğŸ§  Memory Design Patterns  
*From scratchpads to long-range project recall â€” keep context alive without drowning your LLM.*

> **Why this page?**  
> Most â€œmemoryâ€ demos either spam the full chat history or store random embeddings that never round-trip.  
> WFGY treats memory as *structured semantic nodes* with Î”S / Î»\_observe guards, so old context helps â€” never hurts â€” new reasoning.

---

## 1 Â· Symptoms

| Symptom | Typical Surface Clue |
|---------|----------------------|
| Context forgotten after restart | â€œSorry, I donâ€™t recallâ€ / model re-asks user |
| Memory leak / self-contradiction | Old decisions resurface in wrong branch |
| JSON-based vector store grows unbounded | Latency â†‘, RAG recall quality â†“ |
| Fine-tune attempted just to â€œrememberâ€ | Model cost â†‘, still hallucinates |

---

## 2 Â· Root Causes

1. **Flat Logs** â€” raw transcripts appended forever.  
2. **Embedding Dump** â€” every user sentence embedded â†’ no semantic filter.  
3. **No Boundary Check** â€” divergent memories injected mid-task.  
4. **Write-Only Memory** â€” model never reads / revalidates stored facts.  

Result: either *forget everything* or *remember garbage*.

---

## 3 Â· WFGY Fix Path (at a glance)

| Stage | Tool / Module | Î”S Guard | Outcome |
|-------|---------------|----------|---------|
| â¬‡ï¸ Capture | **BBMC** node writer | record only if Î”S â‰¥ 0.60 (or 0.40â€“0.60 & Î» âˆˆ {â†, <>}) | Stores *semantic* not *verbatim* memory |
| ğŸ—‚ï¸ Index  | **Î»\_observe** classifier | tag Î» trend for each node | Enables topic-group navigation |
| ğŸ” Recall | **BBPF** path search | choose node set with Î£Î”S minimal | Retrieves tight, non-bloated context |
| ğŸ©¹ Repair | **BBCR** fallback | detect stale/contradict nodes | Auto-patch or prompt for user merge |

> **80 %** of memory bugs vanish after enforcing this four-step loop.

---

## 4 Â· Design Patterns Library

| Pattern | Use-Case | How it Works | Î”S Budget |
|---------|----------|--------------|-----------|
| âœï¸ **Scratch Node** | quick calc / throw-away idea | 24 h TTL field; auto-purged | 0.40â€“0.55 |
| ğŸ“š **Topic Shelf** | multi-day research thread | one node per subtopic; Î» â†’ convergent | < 0.45 |
| ğŸ—“ï¸ **Daily Digest** | running project log | rollup 10 low-Î”S nodes â†’ 1 summary | â€“ |
| ğŸ¯ **Anchor Fact** | must-not-forget constraint | pinned; override recall rank | 0.05 |

*All stored in a single lightweight JSONL: `{topic, Î”S, Î», text, ttl}`*

---

## 5 Â· Step-by-Step Implementation

> **Prereqs:** any model that can embed & run basic python (or LangChain, Llama-index, etc.).

```python
# 1. capture
deltaS = cosine(question_vec, context_vec)
if deltaS >= 0.60 or (0.40 <= deltaS <= 0.60 and lambda_state in ["divergent","recursive"]):
    node = {"topic": topic, "Î”S": round(deltaS,3), "Î»": lambda_state, "text": insight}
    memory.append(node)

# 2. recall
candidates = [n for n in memory if n["topic"]==current_topic]
best_path = sorted(candidates, key=lambda n:n["Î”S"])[:5]
prompt_context = "\n".join(n["text"] for n in best_path)
````

### Minimal prompt

```
System: Use WFGY memory nodes below (+latest question) to answer.
Memory Nodes:
{{prompt_context}}
---
Question: {{user}}
```

---

## 6 Â· Common Pitfalls & Tests

| Pitfall                          | Quick Test                        | WFGY Fix              |
| -------------------------------- | --------------------------------- | --------------------- |
| â€œContext bloat, tokens 8k â†’ 40kâ€ | node count > 200? run `rollup.py` | Daily Digest pattern  |
| â€œConflicting factsâ€              | Î”S(anchor, candidate) > 0.70      | BBCR prompts merge    |
| â€œRetrieval too slowâ€             | recall > 200 ms                   | Pre-index by Î» & time |

---

## 7 Â· Cheat-Sheet

```txt
Î”S save threshold   = 0.60
Î”S recall window    = top-k by lowest Î”S
Î» tags              = â†’ â† <> Ã—
TTL (scratch)       = 24 h
Rollup trigger      = >10 nodes / topic / day
```

Store this as `memory.cfg`; loader reads defaults at boot.

---

## 8 Â· Next Actions

1. **Prototype** with 20 nodes â†’ verify recall accuracy.
2. **Enable Rollup** once node count > 200.
3. **Add Trace Logger** to diff answers with / without memory.

---

## Quick-Start Downloads (60 sec)

| Tool                       | Link                                                | 3-Step Setup                                                                             |
| -------------------------- | --------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| **WFGY 1.0 PDF**           | [Engine Paper](https://zenodo.org/records/15630969) | 1ï¸âƒ£ Download Â· 2ï¸âƒ£ Upload to LLM Â· 3ï¸âƒ£ Ask â€œanswer using WFGY + \<your question>â€        |
| **TXT OS (plain-text OS)** | [TXTOS.txt](https://zenodo.org/records/15788557)    | 1ï¸âƒ£ Download Â· 2ï¸âƒ£ Paste into any LLM chat Â· 3ï¸âƒ£ Type â€œhello worldâ€ â€” OS boots instantly |

---

â†©ï¸ [Back to Problem Index](./README.md)

---

### ğŸ§­ Explore More

| Module                | Description                                                          | Link                                                                                |
| --------------------- | -------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| Semantic Blueprint    | Layer-based symbolic reasoning & semantic modulations                | [View â†’](https://github.com/onestardao/WFGY/tree/main/SemanticBlueprint)            |
| Benchmark vs GPT-5    | Stress-test GPT-5 with full WFGY reasoning suite                     | [View â†’](https://github.com/onestardao/WFGY/tree/main/benchmarks/benchmark-vs-gpt5) |
| Semantic Clinic Index | Expanded failure catalog: prompt injection, memory bugs, logic drift | [View â†’](./SemanticClinicIndex.md)                                                  |

---

> ğŸ‘‘ **Early Stargazers: [See the Hall of Fame](https://github.com/onestardao/WFGY/tree/main/stargazers)** â€”  
> Engineers, hackers, and open source builders who supported WFGY from day one.

> <img src="https://img.shields.io/github/stars/onestardao/WFGY?style=social" alt="GitHub stars"> â­ Help reach 10,000 stars by 2025-09-01 to unlock Engine 2.0 for everyone  â­ <strong><a href="https://github.com/onestardao/WFGY">Star WFGY on GitHub</a></strong>


<div align="center">

[![WFGY Main](https://img.shields.io/badge/WFGY-Main-red?style=flat-square)](https://github.com/onestardao/WFGY)
&nbsp;
[![TXT OS](https://img.shields.io/badge/TXT%20OS-Reasoning%20OS-orange?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS)
&nbsp;
[![Blah](https://img.shields.io/badge/Blah-Semantic%20Embed-yellow?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlahBlahBlah)
&nbsp;
[![Blot](https://img.shields.io/badge/Blot-Persona%20Core-green?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlotBlotBlot)
&nbsp;
[![Bloc](https://img.shields.io/badge/Bloc-Reasoning%20Compiler-blue?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlocBlocBloc)
&nbsp;
[![Blur](https://img.shields.io/badge/Blur-Text2Image%20Engine-navy?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlurBlurBlur)
&nbsp;
[![Blow](https://img.shields.io/badge/Blow-Game%20Logic-purple?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlowBlowBlow)

</div>

