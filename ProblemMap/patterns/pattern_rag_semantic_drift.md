# Pattern ‚Äî RAG Semantic Drift (No.1)

**Scope**  
Answers **sound plausible** but are **not grounded** in retrieved evidence, or they thread together semantically related but **non-authoritative** chunks. Common in long, generic corpora and when chunk borders split **entity** from its **constraints**.

**Why it matters**  
Semantic drift is the primary source of silent hallucinations. It evades naive keyword checks and only shows up when you enforce grounding, citations, and refusal behavior.

> Quick nav: [Patterns Index](./README.md) ¬∑ Examples: 
> [Example 01](../examples/example_01_basic_fix.md) ¬∑ 
> [Example 03](../examples/example_03_pipeline_patch.md) ¬∑ 
> [Eval: Precision & CHR](../eval/eval_rag_precision_recall.md)

---

## 1) Signals & fast triage

**You likely have this if:**
- The model answers without `citations` **or** cites chunks that don‚Äôt contain the claim.
- Changing chunk size flips answers from correct ‚Üí plausible-but-wrong.
- Retrieving more chunks **degrades** precision (tail noise stitching).
- Reports from Example 02 label many cases as `generation_drift`.

**Deterministic checks (no LLM needed):**
- **Template compliance:** output has `citations: [id,...]` **or** equals `not in context`.
- **Citation overlap:** cited ids ‚äÜ retrieved ids.
- **Containment:** any ‚â•5-char phrase from answer appears verbatim in evidence.

> Use `reflect.py` / `reflect.mjs` from **Example 02**; the label `generation_drift` is your smoking gun.

---

## 2) Minimal reproducible case

Create `data/chunks.json`:

```json
[
  {"id":"p1#1","text":"X is a constrained mapping used in the alpha protocol."},
  {"id":"p1#2","text":"Constraints: X must preserve ordering and reject null keys."},
  {"id":"p2#1","text":"Y is a generalized mapping often compared to X in blogs."}
]
````

Two questions:

* **Q1**: ‚ÄúWhat is X?‚Äù ‚Üí correct chunk is `p1#1`, optionally `p1#2` for constraints.
* **Q2**: ‚ÄúList X‚Äôs constraints.‚Äù ‚Üí correct chunk is `p1#2`.

**Repro:**
Run **Example 01** but increase top-k to 12 without rerank or knee cut. You‚Äôll see the model stitch `p2#1` into the answer when it shouldn‚Äôt.

---

## 3) Root causes

* **Chunk boundary split**: entity in one chunk, constraints in another; retrieval returns only one half.
* **Tail noise**: irrelevant but semantically nearby chunks flood the prompt.
* **Index/schema drift**: embedding model or normalization mismatch yields incomparable scores (see Example 05).
* **Prompt freedom**: no evidence-only contract; model ‚Äúfills gaps‚Äù from prior knowledge.
* **Query parsing**: multi-intent queries aren‚Äôt decomposed; the pipeline answers the wrong sub-intent first.

---

## 4) Standard fix (ordered, minimal, measurable)

**Step 1 ‚Äî Enforce guard** (Example 01)

* Template: evidence-only + refusal token.
* Require `citations: [id,...]`. Treat missing citations as **failure**.

**Step 2 ‚Äî Stabilize retrieval** (Example 03)

* Candidate pools: BM25 **and** embeddings ‚Üí **intersection**, union fallback only if needed.
* Rerank by cosine; **knee cutoff**; keep **top-8**.
* Shrink chunk size so **entity + constraints** co-locate.

**Step 3 ‚Äî Validate index** (Example 05)

* Refuse queries when manifest mismatches runtime.
* Rebuild and re-baseline recall\@k.

**Step 4 ‚Äî Add acceptance gate** (Example 04)

* Scholar emits claim+citations inside **scope.allowed\_ids**; Auditor validates.
* If `INVALID` or `NOT_IN_CONTEXT` ‚Üí do **not** emit text.

**Step 5 ‚Äî Evaluate** (Example 08)

* Track Precision (answered), Over/Under-refusal, Citation Hit Rate.
* Compare before/after; lock gates in CI.

---

## 5) ‚ÄúGood‚Äù vs ‚ÄúBad‚Äù outputs (deterministic examples)

**Good (answerable):**

```
- claim: X is a constrained mapping used in the alpha protocol.
- citations: [p1#1]
```

**Good (answerable with constraints):**

```
- claim: X preserves ordering and rejects null keys.
- citations: [p1#2]
```

**Good (unanswerable):**

```
not in context
```

**Bad (semantic drift):**

```
- claim: X is similar to Y and generally used in blogs.
- citations: [p2#1]     # off-topic; no constraints; not authoritative
```

---

## 6) Acceptance criteria (ship/no-ship)

A response **may ship** only if **all** hold:

1. Output equals refusal token **or** includes `citations` list.
2. `citations ‚äÜ retrieved_ids` and **containment** passes.
3. If multi-agent is used, **Auditor** verdict is `VALID`.
4. **Eval gates**:

   * Precision (answered) ‚â• 0.80
   * Under-refusal ‚â§ 0.05
   * Citation Hit Rate ‚â• 0.75

Otherwise ‚Üí refuse, retry retrieval, or escalate UX.

---

## 7) Prevention (contracts & defaults)

* **Chunking contract**

  * Target **250‚Äì400 tokens**; never exceed 512 without reason.
  * Prefer **semantic paragraph + header** over fixed windows when regulations/constraints exist.
  * Include stable `id`, `source`, `page`, `version`.

* **Retrieval defaults**

  * Pools: `TOPK_LEX=40`, `TOPK_SEM=40` ‚Üí **intersection**, fallback to union if `|‚à©| < 8`.
  * Rerank: cosine vs query; keep **top-8**; apply **knee cut**.

* **Prompt contract**

  * Evidence sandbox; forbid links/tools; **JSON-only** schema.
  * Refusal token: `not in context` (exact string).

* **Index manifest**

  * Pin `embedding.model`, `dimension`, `normalized`, `metric`, `chunker.version`.
  * Validator must **abort** requests on mismatch.

---

## 8) Debug workflow (10 minutes)

1. Run **Example 02** and export `runs/report.jsonl`.
2. Filter `generation_drift` cases.
3. Inspect retrieved ids vs cited ids; if mismatch ‚Üí go to Example 03.
4. If retrieval is fine but answer still floats ‚Üí tighten guard (Example 01) and gate with Auditor (Example 04).
5. Re-score with **Example 08**; commit the `eval/report.md` diff.

---

## 9) Common traps & fixes

* **Raising top-k ‚Äúto be safe‚Äù** ‚Üí increases tail noise ‚Üí worse drift. Fix with **intersection + knee cut**.
* **Large chunks to ‚Äúadd context‚Äù** ‚Üí model stops citing; answers look right but are ungrounded. **Shrink** and **co-locate** constraints.
* **Mixing indices** (different models/dims) across environments ‚Üí incomparable scores. **Validate manifest**; rebuild.
* **Treating refusal as failure** ‚Üí trains teams to prefer bad answers. Count correct refusals as **success**.

---

## 10) Minimal checklist (copy into PR)

* [ ] Guarded template enforced; refusal token exact match.
* [ ] Intersection+rÃ≤eÃ≤rÃ≤aÃ≤nÃ≤kÃ≤ with knee cut; top-8.
* [ ] Manifest validated at boot; readiness flips only after sentinel passes.
* [ ] Auditor gate in place (if multi-agent).
* [ ] Eval gates pass: Precision / Under-refusal / CHR thresholds.

---

## References to hands-on examples

* **Example 01** ‚Äî Guarded template + trace
* **Example 02** ‚Äî Reflection triage (`generation_drift`)
* **Example 03** ‚Äî Intersection + Rerank + Knee
* **Example 04** ‚Äî Multi-agent acceptance gate
* **Example 05** ‚Äî Manifest & index repair
* **Example 08** ‚Äî Quality scoring & CI gates

---


### üß≠ Explore More

| Module                | Description                                              | Link     |
|-----------------------|----------------------------------------------------------|----------|
| WFGY Core             | Standalone semantic reasoning engine for any LLM         | [View ‚Üí](https://github.com/onestardao/WFGY/tree/main/core/README.md) |
| Problem Map 1.0       | Initial 16-mode diagnostic and symbolic fix framework    | [View ‚Üí](https://github.com/onestardao/WFGY/tree/main/ProblemMap/README.md) |
| Problem Map 2.0       | RAG-focused failure tree, modular fixes, and pipelines   | [View ‚Üí](https://github.com/onestardao/WFGY/blob/main/ProblemMap/rag-architecture-and-recovery.md) |
| Semantic Clinic Index | Expanded failure catalog: prompt injection, memory bugs, logic drift | [View ‚Üí](https://github.com/onestardao/WFGY/blob/main/ProblemMap/SemanticClinicIndex.md) |
| Semantic Blueprint    | Layer-based symbolic reasoning & semantic modulations   | [View ‚Üí](https://github.com/onestardao/WFGY/tree/main/SemanticBlueprint/README.md) |
| Benchmark vs GPT-5    | Stress test GPT-5 with full WFGY reasoning suite         | [View ‚Üí](https://github.com/onestardao/WFGY/tree/main/benchmarks/benchmark-vs-gpt5/README.md) |


---

> üëë **Early Stargazers: [See the Hall of Fame](https://github.com/onestardao/WFGY/tree/main/stargazers)** ‚Äî  
> Engineers, hackers, and open source builders who supported WFGY from day one.

> <img src="https://img.shields.io/github/stars/onestardao/WFGY?style=social" alt="GitHub stars"> ‚≠ê Help reach 10,000 stars by 2025-09-01 to unlock Engine 2.0 for everyone  ‚≠ê **[Star WFGY on GitHub](https://github.com/onestardao/WFGY)**


<div align="center">

[![WFGY Main](https://img.shields.io/badge/WFGY-Main-red?style=flat-square)](https://github.com/onestardao/WFGY)
&nbsp;
[![TXT OS](https://img.shields.io/badge/TXT%20OS-Reasoning%20OS-orange?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS)
&nbsp;
[![Blah](https://img.shields.io/badge/Blah-Semantic%20Embed-yellow?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlahBlahBlah)
&nbsp;
[![Blot](https://img.shields.io/badge/Blot-Persona%20Core-green?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlotBlotBlot)
&nbsp;
[![Bloc](https://img.shields.io/badge/Bloc-Reasoning%20Compiler-blue?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlocBlocBloc)
&nbsp;
[![Blur](https://img.shields.io/badge/Blur-Text2Image%20Engine-navy?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlurBlurBlur)
&nbsp;
[![Blow](https://img.shields.io/badge/Blow-Game%20Logic-purple?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlowBlowBlow)

</div>

