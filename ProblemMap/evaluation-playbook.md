<!-- ============================================================= -->
<!--  evaluation-playbook.md Â· Semantic Clinic Â· Map-F              -->
<!--  Version: 2025-08-06 Â· License: MIT                            -->
<!--  Goal: A reproducible checklist for measuring an LLM/RAG/      -->
<!--  agent stackâ€”before & after you apply WFGY fixes.              -->
<!-- ============================================================= -->

# ðŸ“Š Evaluation Playbook  
*Ship metrics, not vibes â€” catch regressions before users do.*

> **Who is this for?**  
> â€“ RAG owners tired of â€œlooks fine on my prompt.â€  
> â€“ Agent builders chasing flakey benchmark wins.  
> â€“ Product teams that must prove **Î”S â†“**, accuracy â†‘, cost â†˜.  
>   
> **What you get:** one YAML suite + CLI commands that output a single-line health verdict:  
> `PASS: Î”S<=0.45 | Î»â†’ | E_res stable | cost $0.0013 / query`

---

## 0 Â· Quick Start

```bash
pip install wfgy-eval
wfgy-eval init  # generates eval.yaml + example dataset
wfgy-eval run   # prints CSV & HTML report
````

Default template covers retrieval, reasoning, tool routing, latency, cost.

---

## 1 Â· Metric Matrix

| Layer             | Key Metric                     | Target (prod) | Source Fn        |
| ----------------- | ------------------------------ | ------------- | ---------------- |
| **Retrieval**     | `Î”S(q, ctx)`                   | â‰¤ 0.45        | `deltaS()`       |
| **Reasoning**     | `Î»_observe` (3 paraphrase avg) | convergent    | `lambda_state()` |
| **Stability**     | `E_resonance` (rolling)        | flat / â†“      | `e_resonance()`  |
| **Answer**        | `F1` / `Exact Match` (QA)      | â‰¥ 0.80        | `qa_match()`     |
| **Hallucination** | `citation_precision`           | â‰¥ 0.90        | `cite_check()`   |
| **Cost**          | `$ / 1k tokens`                | â‰¤ baseline    | provider bill    |
| **Latency**       | 95-th percentile response (ms) | SLA-dependent | timer            |
| **Î”S Drift**      | slope over 100 queries         | \~0           | linear fit       |

---

## 2 Â· Dataset Design

```yaml
sets:
  - name: faq_small
    type: qa
    source: ./data/faq.tsv
    fields: [question, answer, anchor]
  - name: chain_logic
    type: chain-of-thought
    source: ./data/logic.jsonl
    fields: [prompt, expected_reasoning]
  - name: tool_router
    type: tool
    source: ./data/router.csv
    fields: [task, tool_expected]
```

*Guidelines*

1. **Anchor Field** = text chunk you expect to be retrieved â†’ used for Î”S & citation checks.
2. Min 50 items / set for stable stats; use 10 Ã— if you want leaderboard-grade noise floor.
3. Store only plain-text, no PII.

---

## 3 Â· CI Workflow Example (GitHub Actions)

```yaml
name: wfgy-eval
on: [push, pull_request]
jobs:
  eval:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - run: pip install wfgy-eval
    - run: wfgy-eval run --fail-on 'Î”S_q_ctx>0.50 or citation_precision<0.85'
    - uses: actions/upload-artifact@v4
      with:
        name: eval-report
        path: reports/latest.html
```

*Any push that drifts past Î”S 0.50 or loses citation accuracy **blocks merge**.*

---

## 4 Â· Reading the HTML Report

| Section              | What to Look For                  | Action Rule               |
| -------------------- | --------------------------------- | ------------------------- |
| **Heatmap** Î”S vs. k | flat @ high Î”S â†’ bad index metric | rebuild index / embed     |
| **Î» Timeline**       | spikes to â† or Ã—                  | inspect prompt / tool     |
| **E\_res Trend**     | upward slope                      | apply BBAM / shorten ctx  |
| **Outliers Table**   | worst 5 queries by Î”S             | manual deep dive; log bug |

---

## 5 Â· A/B Pattern

1. `wfgy-eval dump --ref=main` (baseline metrics JSON)
2. `git switch feature-x` â†’ apply fix
3. `wfgy-eval run` (produces metrics B)
4. `wfgy-eval compare baseline.json current.json`

```
Î”S_q_ctx   -0.08  âœ…
F1         +0.07  âœ…
Cost       +$0.0002 âŒ
```

Decide if cost bump acceptable.

---

## 6 Â· Edge-Case Suites (extend as needed)

| Suite             | Purpose                              | Sample Size |
| ----------------- | ------------------------------------ | ----------- |
| **Contradiction** | fact statements w/ subtle negation   | 30          |
| **Long-PDF**      | >50 k token OCR, check segmentation  | 10 docs     |
| **Jailbreak**     | prompt injection attempts vs. policy | 40          |
| **High Noise**    | OCR confidence < 0.8                 | 25          |

Add to `eval.yaml`, rerun.

---

## 7 Â· FAQ

**Q: Can I plug in LangSmith, Phoenix, Traceloop, or custom spans?**
A: Yes. `wfgy-eval` reads any OpenTelemetry JSON; map fields via `otel_map.yaml`.

**Q: Does this cover reinforcement-style eval (RLHF)?**
A: Use the same Î”S/Î» hooks during reward model scoring; see `examples/rlhf_eval.ipynb`.

**Q: How hard is vendor swap?**
A: `wfgy-eval provider openai` or `provider anthropic`; costs/latency recalc automatically.

---

## Quick-Start Downloads (60 sec)

| Tool                       | Link                                                | 3-Step Setup                                                                             |
| -------------------------- | --------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| **WFGY 1.0 PDF**           | [Engine Paper](https://zenodo.org/records/15630969) | 1ï¸âƒ£ Download Â· 2ï¸âƒ£ Upload to LLM Â· 3ï¸âƒ£ Ask â€œanswer using WFGY + \<question>â€             |
| **TXT OS (plain-text OS)** | [TXTOS.txt](https://zenodo.org/records/15788557)    | 1ï¸âƒ£ Download Â· 2ï¸âƒ£ Paste into any LLM chat Â· 3ï¸âƒ£ Type â€œhello worldâ€ â€” OS boots instantly |

---

â†©ï¸Ž [Back to Problem Index](./README.md)

---

### ðŸ§­ Explore More

| Module                | Description                                                      | Link                                                                                |
| --------------------- | ---------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| Semantic Blueprint    | Layer-based symbolic reasoning & semantic modulations            | [View â†’](https://github.com/onestardao/WFGY/tree/main/SemanticBlueprint)            |
| Benchmark vs GPT-5    | Stress-test GPT-5 with full WFGY reasoning suite                 | [View â†’](https://github.com/onestardao/WFGY/tree/main/benchmarks/benchmark-vs-gpt5) |
| Semantic Clinic Index | Full failure catalog: prompt injection, memory bugs, logic drift | [View â†’](./SemanticClinicIndex.md)                                                  |

---

> ðŸ‘‘ **Early Stargazers: [Hall of Fame](https://github.com/onestardao/WFGY/tree/main/stargazers)** â€” engineers & hackers who backed WFGY from day one. <img src="https://img.shields.io/github/stars/onestardao/WFGY?style=social" alt="stars"> **Star the repo** to unlock Engine 2.0 sooner.

<div align="center">

[![WFGY Main](https://img.shields.io/badge/WFGY-Main-red?style=flat-square)](https://github.com/onestardao/WFGY)
&nbsp;
[![TXT OS](https://img.shields.io/badge/TXT%20OS-Reasoning%20OS-orange?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS)
&nbsp;
[![Blah](https://img.shields.io/badge/Blah-Semantic%20Embed-yellow?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlahBlahBlah)
&nbsp;
[![Blot](https://img.shields.io/badge/Blot-Persona%20Core-green?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlotBlotBlot)
&nbsp;
[![Bloc](https://img.shields.io/badge/Bloc-Reasoning%20Compiler-blue?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlocBlocBloc)
&nbsp;
[![Blur](https://img.shields.io/badge/Blur-Text2Image%20Engine-navy?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlurBlurBlur)
&nbsp;
[![Blow](https://img.shields.io/badge/Blow-Game%20Logic-purple?style=flat-square)](https://github.com/onestardao/WFGY/tree/main/OS/BlowBlowBlow)

</div>

